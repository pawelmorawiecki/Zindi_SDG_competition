{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchtext import data\n",
    "import spacy\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SEED = 12345\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def text_clean(text):\n",
    "    text = re.sub(r'[^A-Za-z0-9]+', ' ', text) # remove non alphanumeric character\n",
    "    text = re.sub(r'https?:/\\/\\S+', ' ', text) # remove links\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!python -m spacy download en #if needed, please uncomment and download english model for spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tokenizer function using spacy\n",
    "nlp = spacy.load('en',disable=['parser', 'tagger', 'ner'])\n",
    "min_len = 5\n",
    "def tokenizer(s): \n",
    "    tokenized = [w.text.lower() for w in nlp(text_clean(s))]\n",
    "    if len(tokenized) < min_len: #make sure that length is at least 6 \n",
    "        tokenized += ['<pad>'] * (min_len - len(tokenized))\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "txt_field = data.Field(sequential=True, \n",
    "                       tokenize=tokenizer, \n",
    "                       use_vocab=True)\n",
    "\n",
    "label_field = data.Field(sequential=False, \n",
    "                         use_vocab=False, \n",
    "                         pad_token=None, \n",
    "                         unk_token=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_val_fields = [\n",
    "    ('Unique ID', None), #this feature is not processed \n",
    "    ('Type', None), #this feature is not processed \n",
    "    ('Text', txt_field), #process it as a text\n",
    "    ('l_3.1.1', label_field), #process it as a label\n",
    "    ('l_3.1.2', label_field),\n",
    "    ('l_3.2.1', label_field),\n",
    "    ('l_3.2.2', label_field),\n",
    "    ('l_3.3.1', label_field),\n",
    "    ('l_3.3.2', label_field),\n",
    "    ('l_3.3.3', label_field),\n",
    "    ('l_3.3.4', label_field),\n",
    "    ('l_3.3.5', label_field),\n",
    "    ('l_3.4.1', label_field),\n",
    "    ('l_3.4.2', label_field),\n",
    "    ('l_3.5.1', label_field),\n",
    "    ('l_3.5.2', label_field),\n",
    "    ('l_3.6.1', label_field),\n",
    "    ('l_3.7.1', label_field),\n",
    "    ('l_3.7.2', label_field),\n",
    "    ('l_3.8.1', label_field),\n",
    "    ('l_3.8.2', label_field),\n",
    "    ('l_3.9.1', label_field),\n",
    "    ('l_3.9.2', label_field),\n",
    "    ('l_3.9.3', label_field),\n",
    "    ('l_3.a.1', label_field),\n",
    "    ('l_3.b.1', label_field),\n",
    "    ('l_3.b.2', label_field),\n",
    "    ('l_3.b.3', label_field),\n",
    "    ('l_3.c.1', label_field),\n",
    "    ('l_3.d.1', label_field)    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data, val_data = data.TabularDataset.splits(path='./',\n",
    "                                                  format='csv', \n",
    "                                                  train='train_clean.csv', \n",
    "                                                  validation='my_val_set.csv', \n",
    "                                                  fields=train_val_fields, \n",
    "                                                  skip_header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build the vocabulary and load the pre-trained word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#txt_field.build_vocab(train_data, val_data, max_size=25000, vectors=\"glove.6B.300d\")\n",
    "txt_field.build_vocab(train_data, val_data, max_size=25000, vectors=\"fasttext.en.300d\")\n",
    "label_field.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "train_iterator, valid_iterator = data.BucketIterator.splits(\n",
    "                                (train_data, val_data), \n",
    "                                sort_key=lambda x: len(x.Text),\n",
    "                                sort_within_batch=True,\n",
    "                                batch_size=BATCH_SIZE, \n",
    "                                device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_target_labels(batch, device, train_val_fields):\n",
    "    target_labels = np.zeros((len(batch),0))\n",
    "    for i in range(27): #27 labels \n",
    "        label_name = train_val_fields[i+3][0] #get a name of label, label starts from index=3 \n",
    "        single_label = getattr(batch,label_name).cpu().numpy()\n",
    "        single_label.shape = (len(batch),1)\n",
    "        target_labels = np.hstack((target_labels,single_label))\n",
    "    return torch.from_numpy(target_labels).to(device)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, n_filters, filter_sizes, output_dim, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.conv_0 = nn.Conv2d(in_channels=1, out_channels=n_filters, kernel_size=(filter_sizes[0],embedding_dim))\n",
    "        self.conv_1 = nn.Conv2d(in_channels=1, out_channels=n_filters, kernel_size=(filter_sizes[1],embedding_dim))\n",
    "        self.conv_2 = nn.Conv2d(in_channels=1, out_channels=n_filters, kernel_size=(filter_sizes[2],embedding_dim))\n",
    "        self.conv_3 = nn.Conv2d(in_channels=1, out_channels=n_filters, kernel_size=(filter_sizes[3],embedding_dim))\n",
    "        self.conv_4 = nn.Conv2d(in_channels=1, out_channels=n_filters, kernel_size=(filter_sizes[4],embedding_dim))\n",
    "    \n",
    "        self.fc = nn.Linear(len(filter_sizes)*n_filters, output_dim)    \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        #x = [sent len, batch size]\n",
    "        \n",
    "        x = x.permute(1, 0)\n",
    "                \n",
    "        #x = [batch size, sent len]\n",
    "        \n",
    "        embedded = self.embedding(x)\n",
    "                \n",
    "        #embedded = [batch size, sent len, emb dim]\n",
    "        \n",
    "        embedded = embedded.unsqueeze(1)\n",
    "        #print (embedded.size())\n",
    "        #embedded = [batch size, 1, sent len, emb dim]\n",
    "        \n",
    "        conved_0 = F.relu(self.conv_0(embedded).squeeze(3))\n",
    "        conved_1 = F.relu(self.conv_1(embedded).squeeze(3))\n",
    "        conved_2 = F.relu(self.conv_2(embedded).squeeze(3))\n",
    "        conved_3 = F.relu(self.conv_3(embedded).squeeze(3))\n",
    "        conved_4 = F.relu(self.conv_4(embedded).squeeze(3))\n",
    "        \n",
    "            \n",
    "        #conv_n = [batch size, n_filters, sent len - filter_sizes[n]]\n",
    "        \n",
    "        pooled_0 = F.max_pool1d(conved_0, conved_0.shape[2]).squeeze(2)\n",
    "        pooled_1 = F.max_pool1d(conved_1, conved_1.shape[2]).squeeze(2)\n",
    "        pooled_2 = F.max_pool1d(conved_2, conved_2.shape[2]).squeeze(2)\n",
    "        pooled_3 = F.max_pool1d(conved_3, conved_3.shape[2]).squeeze(2)\n",
    "        pooled_4 = F.max_pool1d(conved_4, conved_4.shape[2]).squeeze(2)\n",
    "        \n",
    "        #pooled_n = [batch size, n_filters]\n",
    "        \n",
    "        cat = self.dropout(torch.cat((pooled_0, pooled_1, pooled_2, pooled_3, pooled_4), dim=1))\n",
    "\n",
    "        #cat = [batch size, n_filters * len(filter_sizes)]\n",
    "            \n",
    "        return self.fc(cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "INPUT_DIM = len(txt_field.vocab)\n",
    "EMBEDDING_DIM = 300\n",
    "N_FILTERS = 300\n",
    "FILTER_SIZES = [1,2,3,4,5]\n",
    "OUTPUT_DIM = 27\n",
    "DROPOUT = 0.5\n",
    "\n",
    "\n",
    "model = CNN(INPUT_DIM, EMBEDDING_DIM, N_FILTERS, FILTER_SIZES, OUTPUT_DIM, DROPOUT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load pretrained embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [-0.0653, -0.0930, -0.0176,  ...,  0.1664, -0.1308,  0.0354],\n",
       "        ...,\n",
       "        [ 0.1434,  0.1650, -0.3431,  ...,  0.1982,  0.3606,  0.0768],\n",
       "        [-0.1984,  0.1341, -0.3664,  ...,  0.1997,  0.4324,  0.2593],\n",
       "        [ 0.0289,  0.2313, -0.3855,  ...,  0.1142,  0.2038, -0.3233]])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_embeddings = txt_field.vocab.vectors\n",
    "model.embedding.weight.data.copy_(pretrained_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "#optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "model = model.to(DEVICE)\n",
    "criterion = criterion.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_HW(predictions, target_labels):\n",
    "    #rounded_preds = (torch.sigmoid(predictions)>0.25).float()\n",
    "    rounded_preds = torch.round(torch.sigmoid(predictions))\n",
    "    incorrects = (rounded_preds != target_labels.float())\n",
    "    HW = incorrects.cpu().numpy().sum()\n",
    "    return HW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    metric = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for batch in iterator:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        predictions = model(batch.Text)\n",
    "        target_labels = get_target_labels(batch,DEVICE,train_val_fields)\n",
    "        \n",
    "        loss = criterion(predictions, target_labels.float())\n",
    "        \n",
    "        HW = calculate_HW(predictions, target_labels) #caculate Hamming weight in batch\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        metric = metric + HW \n",
    "        \n",
    "    return epoch_loss/len(iterator), metric/(len(train_data)*27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    metric = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for batch in iterator:\n",
    "\n",
    "            predictions = model(batch.Text)\n",
    "            target_labels = get_target_labels(batch,DEVICE,train_val_fields)\n",
    "        \n",
    "            loss = criterion(predictions, target_labels.float())\n",
    "            \n",
    "            HW = calculate_HW(predictions, target_labels) #caculate Hamming Weight in batch\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            metric = metric + HW\n",
    "        \n",
    "    #return epoch_loss/len(iterator), metric/(len(train_data)*27)\n",
    "    return epoch_loss/len(iterator), metric/(len(val_data)*27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0.03119965690247556\n",
      "0.009608606937488407\n",
      "0.016027373261749744\n",
      "0.0049382716049382715\n",
      "--------------\n",
      "1\n",
      "0.030069927982193358\n",
      "0.009695170963952266\n",
      "0.01240212531411089\n",
      "0.0036087369420702755\n",
      "--------------\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 15\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    train_loss, train_metric = train(model, train_iterator, optimizer, criterion)\n",
    "    valid_loss, valid_metric = evaluate(model, valid_iterator, criterion)\n",
    "    #valid_loss, valid_metric = evaluate(model, train_iterator, criterion)\n",
    "    \n",
    "    #print(f'| Epoch: {epoch+1:02} | Train Loss: {train_loss:.3f} | Val. Loss: {valid_loss:.3f} |')\n",
    "    print (epoch)\n",
    "    print (train_loss)\n",
    "    print (train_metric)\n",
    "    print (valid_loss)\n",
    "    print (valid_metric)\n",
    "    print ('--------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Generating submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_train_clean = pd.read_csv('train_clean.csv', low_memory=False)\n",
    "df_test_clean =  pd.read_csv('test_clean.csv', low_memory=False)\n",
    "df_submission =  pd.read_csv('Devex_submission_format.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def predict(sentence, min_len=6):\n",
    "    #tokenized = [tok.text for tok in nlp.tokenizer(text_clean(sentence))]\n",
    "    tokenized = tokenizer(sentence)\n",
    "    if len(tokenized) < min_len:\n",
    "        tokenized += ['<pad>'] * (min_len - len(tokenized))\n",
    "    indexed = [txt_field.vocab.stoi[t] for t in tokenized]\n",
    "    tensor = torch.LongTensor(indexed).to(DEVICE)\n",
    "    tensor = tensor.unsqueeze(1)\n",
    "    predictions = torch.sigmoid(model(tensor))\n",
    "    preds_rounded = torch.round(predictions)\n",
    "\n",
    "    return preds_rounded.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "for i in range(len(df_submission)):\n",
    "    id = df_submission.at[i,'ID']\n",
    "    index = df_test_clean.index[df_test_clean['Unique ID']==id].tolist()\n",
    "    text = df_test_clean.at[index[0],'Text']\n",
    "    predictions = predict(text,5) #calculate predictions, padded to 5 if needed   \n",
    "    for j in range (1,28): #labels starts from 1 in df_submission\n",
    "        df_submission.iloc[i,j]=predictions.cpu().numpy()[0][j-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_submission = df_submission.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_submission.to_csv('my_submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
